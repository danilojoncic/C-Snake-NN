{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyO9RHuB9O/Ez/yBMu9IRDG0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X9ID8Q1V79dm","executionInfo":{"status":"ok","timestamp":1721910756553,"user_tz":-120,"elapsed":134035,"user":{"displayName":"Danilo Joncic","userId":"12467872683224926268"}},"outputId":"1e1ae549-e9db-47f0-92f6-79b1cc8f6054"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","651/651 [==============================] - 5s 5ms/step - loss: 0.5887 - accuracy: 0.8065 - val_loss: 0.3447 - val_accuracy: 0.8708\n","Epoch 2/50\n","651/651 [==============================] - 3s 4ms/step - loss: 0.3136 - accuracy: 0.8857 - val_loss: 0.2999 - val_accuracy: 0.8878\n","Epoch 3/50\n","651/651 [==============================] - 3s 5ms/step - loss: 0.2801 - accuracy: 0.8964 - val_loss: 0.2821 - val_accuracy: 0.8973\n","Epoch 4/50\n","651/651 [==============================] - 2s 3ms/step - loss: 0.2616 - accuracy: 0.9032 - val_loss: 0.2618 - val_accuracy: 0.9038\n","Epoch 5/50\n","651/651 [==============================] - 3s 4ms/step - loss: 0.2480 - accuracy: 0.9079 - val_loss: 0.2595 - val_accuracy: 0.9005\n","Epoch 6/50\n","651/651 [==============================] - 2s 3ms/step - loss: 0.2358 - accuracy: 0.9110 - val_loss: 0.2459 - val_accuracy: 0.9072\n","Epoch 7/50\n","651/651 [==============================] - 2s 3ms/step - loss: 0.2238 - accuracy: 0.9153 - val_loss: 0.2500 - val_accuracy: 0.9115\n","Epoch 8/50\n","651/651 [==============================] - 3s 5ms/step - loss: 0.2148 - accuracy: 0.9191 - val_loss: 0.2176 - val_accuracy: 0.9215\n","Epoch 9/50\n","651/651 [==============================] - 3s 4ms/step - loss: 0.2047 - accuracy: 0.9235 - val_loss: 0.2087 - val_accuracy: 0.9213\n","Epoch 10/50\n","651/651 [==============================] - 2s 3ms/step - loss: 0.1972 - accuracy: 0.9258 - val_loss: 0.2011 - val_accuracy: 0.9224\n","Epoch 11/50\n","651/651 [==============================] - 2s 3ms/step - loss: 0.1889 - accuracy: 0.9268 - val_loss: 0.2077 - val_accuracy: 0.9241\n","Epoch 12/50\n","651/651 [==============================] - 2s 3ms/step - loss: 0.1820 - accuracy: 0.9296 - val_loss: 0.2062 - val_accuracy: 0.9203\n","Epoch 13/50\n","651/651 [==============================] - 4s 5ms/step - loss: 0.1722 - accuracy: 0.9320 - val_loss: 0.1842 - val_accuracy: 0.9326\n","Epoch 14/50\n","651/651 [==============================] - 2s 4ms/step - loss: 0.1612 - accuracy: 0.9376 - val_loss: 0.1714 - val_accuracy: 0.9305\n","Epoch 15/50\n","651/651 [==============================] - 2s 3ms/step - loss: 0.1548 - accuracy: 0.9385 - val_loss: 0.1825 - val_accuracy: 0.9255\n","Epoch 16/50\n","651/651 [==============================] - 2s 3ms/step - loss: 0.1500 - accuracy: 0.9426 - val_loss: 0.1615 - val_accuracy: 0.9318\n","Epoch 17/50\n","651/651 [==============================] - 3s 4ms/step - loss: 0.1406 - accuracy: 0.9458 - val_loss: 0.1652 - val_accuracy: 0.9322\n","Epoch 18/50\n","651/651 [==============================] - 3s 4ms/step - loss: 0.1343 - accuracy: 0.9474 - val_loss: 0.1424 - val_accuracy: 0.9447\n","Epoch 19/50\n","651/651 [==============================] - 3s 4ms/step - loss: 0.1293 - accuracy: 0.9498 - val_loss: 0.1341 - val_accuracy: 0.9441\n","Epoch 20/50\n","651/651 [==============================] - 2s 3ms/step - loss: 0.1235 - accuracy: 0.9522 - val_loss: 0.1293 - val_accuracy: 0.9457\n","Epoch 21/50\n","651/651 [==============================] - 2s 3ms/step - loss: 0.1167 - accuracy: 0.9557 - val_loss: 0.1229 - val_accuracy: 0.9506\n","Epoch 22/50\n","651/651 [==============================] - 2s 3ms/step - loss: 0.1123 - accuracy: 0.9566 - val_loss: 0.1331 - val_accuracy: 0.9449\n","Epoch 23/50\n","651/651 [==============================] - 2s 4ms/step - loss: 0.1076 - accuracy: 0.9571 - val_loss: 0.1262 - val_accuracy: 0.9478\n","Epoch 24/50\n","651/651 [==============================] - 3s 5ms/step - loss: 0.1041 - accuracy: 0.9595 - val_loss: 0.1132 - val_accuracy: 0.9570\n","Epoch 25/50\n","651/651 [==============================] - 2s 3ms/step - loss: 0.1011 - accuracy: 0.9611 - val_loss: 0.1019 - val_accuracy: 0.9606\n","Epoch 26/50\n","651/651 [==============================] - 2s 3ms/step - loss: 0.0938 - accuracy: 0.9648 - val_loss: 0.1035 - val_accuracy: 0.9616\n","Epoch 27/50\n","651/651 [==============================] - 2s 3ms/step - loss: 0.0926 - accuracy: 0.9640 - val_loss: 0.1233 - val_accuracy: 0.9512\n","Epoch 28/50\n","651/651 [==============================] - 2s 3ms/step - loss: 0.0914 - accuracy: 0.9649 - val_loss: 0.0956 - val_accuracy: 0.9639\n","Epoch 29/50\n","651/651 [==============================] - 3s 5ms/step - loss: 0.0856 - accuracy: 0.9678 - val_loss: 0.0984 - val_accuracy: 0.9641\n","Epoch 30/50\n","651/651 [==============================] - 3s 4ms/step - loss: 0.0797 - accuracy: 0.9708 - val_loss: 0.1036 - val_accuracy: 0.9610\n","Epoch 31/50\n","651/651 [==============================] - 2s 3ms/step - loss: 0.0798 - accuracy: 0.9703 - val_loss: 0.0933 - val_accuracy: 0.9641\n","Epoch 32/50\n","651/651 [==============================] - 2s 3ms/step - loss: 0.0758 - accuracy: 0.9715 - val_loss: 0.0842 - val_accuracy: 0.9674\n","Epoch 33/50\n","651/651 [==============================] - 2s 3ms/step - loss: 0.0682 - accuracy: 0.9733 - val_loss: 0.0805 - val_accuracy: 0.9700\n","Epoch 34/50\n","651/651 [==============================] - 3s 5ms/step - loss: 0.0663 - accuracy: 0.9745 - val_loss: 0.0781 - val_accuracy: 0.9729\n","Epoch 35/50\n","651/651 [==============================] - 2s 4ms/step - loss: 0.0641 - accuracy: 0.9758 - val_loss: 0.0830 - val_accuracy: 0.9698\n","Epoch 36/50\n","651/651 [==============================] - 2s 3ms/step - loss: 0.0645 - accuracy: 0.9750 - val_loss: 0.0698 - val_accuracy: 0.9745\n","Epoch 37/50\n","651/651 [==============================] - 2s 3ms/step - loss: 0.0581 - accuracy: 0.9780 - val_loss: 0.0857 - val_accuracy: 0.9687\n","Epoch 38/50\n","651/651 [==============================] - 2s 3ms/step - loss: 0.0564 - accuracy: 0.9786 - val_loss: 0.0827 - val_accuracy: 0.9687\n","Epoch 39/50\n","651/651 [==============================] - 3s 4ms/step - loss: 0.0554 - accuracy: 0.9794 - val_loss: 0.0613 - val_accuracy: 0.9775\n","Epoch 40/50\n","651/651 [==============================] - 3s 4ms/step - loss: 0.0532 - accuracy: 0.9808 - val_loss: 0.0611 - val_accuracy: 0.9762\n","Epoch 41/50\n","651/651 [==============================] - 2s 3ms/step - loss: 0.0516 - accuracy: 0.9802 - val_loss: 0.0645 - val_accuracy: 0.9752\n","Epoch 42/50\n","651/651 [==============================] - 2s 3ms/step - loss: 0.0481 - accuracy: 0.9811 - val_loss: 0.0582 - val_accuracy: 0.9793\n","Epoch 43/50\n","651/651 [==============================] - 2s 3ms/step - loss: 0.0469 - accuracy: 0.9826 - val_loss: 0.0764 - val_accuracy: 0.9714\n","Epoch 44/50\n","651/651 [==============================] - 2s 4ms/step - loss: 0.0473 - accuracy: 0.9822 - val_loss: 0.0751 - val_accuracy: 0.9712\n","Epoch 45/50\n","651/651 [==============================] - 3s 5ms/step - loss: 0.0416 - accuracy: 0.9844 - val_loss: 0.0677 - val_accuracy: 0.9762\n","Epoch 46/50\n","651/651 [==============================] - 2s 3ms/step - loss: 0.0444 - accuracy: 0.9831 - val_loss: 0.0537 - val_accuracy: 0.9814\n","Epoch 47/50\n","651/651 [==============================] - 2s 3ms/step - loss: 0.0406 - accuracy: 0.9849 - val_loss: 0.0506 - val_accuracy: 0.9814\n","Epoch 48/50\n","651/651 [==============================] - 3s 4ms/step - loss: 0.0402 - accuracy: 0.9843 - val_loss: 0.0547 - val_accuracy: 0.9829\n","Epoch 49/50\n","651/651 [==============================] - 2s 3ms/step - loss: 0.0400 - accuracy: 0.9848 - val_loss: 0.0458 - val_accuracy: 0.9821\n","Epoch 50/50\n","651/651 [==============================] - 3s 5ms/step - loss: 0.0356 - accuracy: 0.9871 - val_loss: 0.0518 - val_accuracy: 0.9848\n","Weights of layer 0 saved to layer_0_weights.csv\n","Biases of layer 0 saved to layer_0_biases.csv\n","Weights of layer 1 saved to layer_1_weights.csv\n","Biases of layer 1 saved to layer_1_biases.csv\n","Weights of layer 2 saved to layer_2_weights.csv\n","Biases of layer 2 saved to layer_2_biases.csv\n","Current Working Directory: /content\n","Files in Current Directory: ['.config', 'layer_0_biases.csv', 'layer_2_biases.csv', 'layer_1_weights.csv', 'layer_0_weights.csv', 'choices.csv', 'layer_2_weights.csv', 'layer_1_biases.csv', 'sample_data']\n","Files for layer 0 saved successfully.\n","Files for layer 1 saved successfully.\n","Files for layer 2 saved successfully.\n"]}],"source":["\n","# Step 2: Load and preprocess the data\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from sklearn.model_selection import train_test_split\n","import os\n","import csv\n","\n","# Load the data\n","filename = 'choices.csv'  # Replace with your file path\n","data = pd.read_csv(filename)\n","\n","# Extract features and labels\n","features = data.iloc[:,:-1].values  # All columns except the first (\"score\") and last (\"choice of direction\")\n","labels = data.iloc[:, -1].values  # Last column is the label\n","\n","# Convert labels to one-hot encoding\n","labels = tf.keras.utils.to_categorical(labels, num_classes=4)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n","\n","# Step 3: Define and train the neural network\n","# Define the neural network model\n","model = Sequential([\n","    Dense(32, input_dim=9, activation='relu'),  # input_dim set to 9\n","    Dense(32, activation='relu'),\n","    Dense(4, activation='softmax')  # 4 output neurons for the 4 possible directions\n","])\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n","\n","# Step 4: Save the weights and biases to CSV files\n","def save_to_csv(filename, data):\n","    with open(filename, 'w', newline='') as file:\n","        writer = csv.writer(file)\n","        for row in data:\n","            writer.writerow(row)\n","\n","for i, layer in enumerate(model.layers):\n","    weights, biases = layer.get_weights()\n","\n","    # Save weights\n","    weights_filename = f'layer_{i}_weights.csv'\n","    save_to_csv(weights_filename, weights)\n","    print(f\"Weights of layer {i} saved to {weights_filename}\")\n","\n","    # Save biases\n","    biases_filename = f'layer_{i}_biases.csv'\n","    save_to_csv(biases_filename, [biases])  # Biases are 1D array, wrap them in a list to save correctly\n","    print(f\"Biases of layer {i} saved to {biases_filename}\")\n","\n","# Step 5: Verify the file location\n","# Verify the current working directory\n","print(\"Current Working Directory:\", os.getcwd())\n","\n","# List files in the current directory\n","print(\"Files in Current Directory:\", os.listdir())\n","\n","# Confirm that the files are saved\n","for i in range(len(model.layers)):\n","    if f'layer_{i}_weights.csv' in os.listdir() and f'layer_{i}_biases.csv' in os.listdir():\n","        print(f\"Files for layer {i} saved successfully.\")\n","    else:\n","        print(f\"Files for layer {i} not found.\")\n"]}]}